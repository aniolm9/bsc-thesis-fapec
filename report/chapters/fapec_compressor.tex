\chapter{The FAPEC Data Compressor}

\acrfull{fapec} is a versatile and efficient data compressor originally designed for space missions \parencite{PaperFAPEC}. \acrshort{fapec}'s main advantages are its high computing performance and its resistance in front of noise or data outliers.

In this chapter we will explore \acrshort{fapec}'s structure and its integration with several decorrelation stages such as the two presented in this work. In order to do that, we will first see the origin of \acrshort{fapec} and its need in space industry.

\section{Background}
Space missions are continuously evolving, hence the amount of data generated is growing. However, onboard computing power and downlink bandwidth are not evolving so fast and they are still quite narrowed. The usual technique to deal with the second limitation is data compression, but it has to be implemented efficiently as we are still limited by the first constraint. Moreover, the chosen algorithm must be reliable, as losing data in this harsh environment could be fatal.

The \acrfull{ccsds} already provides some standards for space data compression. For instance, \acrshort{ccsds} 121.0 for generic lossless data compression, \acrshort{ccsds} 122.0 \parencite{ccsds122} for image data compression, etc. Although they achieve good compression ratios and their overall performance is remarkable, they can be too complex and demanding for some missions with limited hardware, such as cubesats. Besides, having a unique solution for data compression would be better than having different standards for different data types.

In order to work out the previous limitations, \acrshort{fapec} was proposed, featuring:
\begin{itemize}
	\item High computing performance.
	\item Optimization for very low and very high entropy levels.
	\item Several preprocessing stages to suit different types of data.
\end{itemize}

\section{General overview}
The \acrshort{fapec} data compressor is based on two stages: a preprocessing stage followed by the entropy coder. In fact, \acrshort{fapec}'s name comes from this architecture as usually the first stage is some type of predictor which tries to estimate the true samples and generates a prediction error. Then, the error is sent to the entropy coder instead of the original samples (Prediction Error Coder).

Formally, given an input sample $x_i$ and an estimator $\hat{x}_i$, the value sent to the entropy coder is:
\begin{equation}
e_i = x_i - \hat{x}_i
\end{equation}

Restoring the original value in decompression is straightforward:
\begin{equation}
x_i = \hat{x}_i + e_i
\end{equation}

Notice that in a few stages some flags are also sent to the entropy coder.

\begin{figure}[h!]
	\begin{center}
		\scalebox{.565}{\input{images/fapec_structure.tex}}
	\end{center}
	\caption{The \acrshort{fapec} data compressor structure.}
	\label{fig:fapec_structure}
\end{figure}

Where $T()$ is a generic transformation applied to the input sequence.

The main purpose of decorrelation stages in data compressors is to reduce the entropy at the coder input to allow a better performance of the latter. In other words, the preprocessing stage performance is critical in the overall compressor efficiency.

\acrshort{fapec} features several preprocessing algorithms (Fully Adaptive) such as a basic differential, multi-band prediction, etc. In this work we introduce two new stages that also follow the former approach.

\section{Entropy coder}
\acrshort{fapec}'s entropy coder is inspired by the Golomb-Rice coder used in the \acrshort{ccsds} 121.0 standard for lossless data compression. \acrshort{fapec} aims to perform similarly to Rice codes but also fix some of the problems related with them.

In order to achieve a good adaptation in front of outliers, \acrshort{fapec} works on data blocks of, typically, 128 samples. The core analyzes every block of prediction samples and determines the optimum coding tables. Finally, it calls the coding kernel, which generates a variable-length code for every given prediction error. Although \acrshort{fapec} needs larger data blocks compared with \acrshort{ccsds} 121.0, it has the advantage that the maximum code length is limited to less than twice the bit length of the input values.

Previously we have said that \acrshort{fapec}'s core performs a statistical analysis of the prediction errors. If this analysis determines a very high entropy \acrshort{fapec} does not try to encode the values and they are sent to the output as is. On the other hand, the analysis might reveal very low entropy levels (about 90\% of the values being -1, 0 or 1). Assuming a probability of $\frac{1}{3}$ for all of them and sequences of 6 values, there are $3^6$ which can be encoded using 10 bits ($2^9 < 3^6 < 2^{10}$). The average is 1.66 bits per sample, a 95\% of the Shannon limit. Finally, sequences of 5 zeroes are also detected and a run length encoding is performed.
