\chapter{Entropy Coding}
Entropy coding is a lossless data compression scheme based on symbols probability. This concept was first described by Claude E. Shannon in 1948 in his paper \textit{A~Mathematical Theory of Communication} \parencite{Shannon1948}.

Giving an extensive theoretical explanation of Entropy coding would require an entire thesis. Therefore, in this chapter we will define the most basic equations in Information Theory and we will also describe some well-known coding algorithms that will be relevant later. The interested reader should see reference \parencite{cover}.

\section{Information theory basics}
\subsection{Shannon entropy}
Given a discrete random variable $X$ with probability function $p(x)$, its Shannon entropy is defined as:

\begin{equation}
H(X) = - \sum_{i=1}^{n} p(x_i) \cdot \log_2 p(x_i) 
\end{equation}

This result represents the average level of information (or uncertainty) of the random variable $X$. It is expressed in \textit{bits}. Sometimes entropy is defined using the natural logarithm. In those situations the result is in \textit{nats}.

\subsubsection{Shannon's source coding theorem}
Shannon's source coding theorem establishes the theoretical limits to data compression, and a different meaning to Shannon entropy defined above. Formally:

\begin{theorem}
Let $L_n^*$ be the expected codeword length per symbol of an optimal n-th order lossless data compression code (in bits/symbol). Let $(X_1, X_2, \dots, X_n )$ be a sequence of symbols from a stochastic process $X$. Then,

\begin{equation}
	\frac{H(X_1, X_2, \dots, X_n )}{n} \leq L_n^* < \frac{H(X_1, X_2, \dots, X_n )}{n} + \frac{1}{n}
\end{equation}

If $X$ is a stationary stochastic process,
\begin{equation}
	\lim_{n \to \infty} L_n^* = H(X)
\end{equation}
\end{theorem}

The previous theorem reveals a new interpretation of Shannon entropy: it is the average number of bits per symbol required to encode it.

\subsection{Differential entropy}
Given a continuous random variable $X$ with a density $f(x)$ supported in $S$, its differential entropy is defined as:
\begin{equation} \label{eq:differential_entropy}
h(x) = - \int_{S}^{} f(x) \ln f(x) dx
\end{equation}

Notice that we have defined differential entropy using the natural logarithm, hence the result shall be expressed in \textit{nats}.

In some cases computing an explicit probability density function is too hard or even impossible. In these cases, an equivalent definition in terms of the quantile function $Q(p)$ can be used (see appendix \ref{entropy_proof}):
\begin{equation} \label{eq:differential_entropy_q}
h(p) = \int_{0}^{1} \ln Q'(p) dp
\end{equation}

This equation is specially useful in some entropy estimation techniques \parencite{Vasicek}.

\subsubsection{Entropy estimation}
Estimating differential entropy is useful in many fields such as image analysis, manifold learning or speech recognition. In this work we will use plug-in methods \parencite{EntropyEstimation} to estimate differential entropy. The result will be compared with the maximum entropy (see below) to analyze the effectiveness of the designed stages.

\subsubsection{Maximum entropy}
Among all the real-valued distributions supported on $(-\infty, +\infty)$ with a specified variance $\sigma^2$, the normal distribution $N(\mu, \sigma^2)$ has maximum entropy:
\begin{equation}
h(x) = \frac{1}{2} \ln \left(2\sigma^2\pi\right) + \frac{1}{2} = \ln\left(\sqrt{2\pi e \sigma^2}\right)
\end{equation}

\section{Coding techniques}
There exist several techniques to assign bits to symbols. In general, one would be interested in a coding scheme which gives codewords with a mean length as close as possible to the Shannon limit. However, sometimes achieving the optimality might be too demanding, so other faster but suboptimal techniques have been developed.

In this section we will describe Golomb coding, a known technique used by several standards such as \acrshort{flac} or \acrshort{ccsds} 121.0. Besides, the \acrshort{fapec} entropy coder is highly inspired in Rice codes, a subset of the Golomb codes.

Before continuing, it is important to observe that there exist other entropy coding techniques which are more common than Golomb, for instance Huffman coding \parencite{cover}, arithmetic coding \parencite{MacKay} or, more recently, the Asymmetric Numeral System \parencite{ans}.

\subsection{Golomb coding} \label{golomb-coding}
Golomb codes were first proposed by Solomon W. Golomb in 1966 in his article \textit{Run-Length Encodings} \parencite{Golomb1966}. In this paper, Golomb proposes these codes as a way to encode events which follow a geometric (or exponential) distribution. For these distributions, Golomb codes are optimal \parencite{OptimalRice} \parencite{OptimalGeometric} and fast to calculate. Simplifying, Golomb coding is highly suitable for situations where small values have a bigger probability than large values.

\subsubsection{Encoding procedure}
Let $N$ be the number to encode. Then, its Golomb code is:

\begin{algorithm}[H]
	\caption{Golomb encoding procedure}
	\SetAlgoLined
	\textbf{Input:} $N$\\
	\textbf{Define:} $M,q,r \in \mathbb{N}$, \quad $b = \left\lceil \log_2(M) \right\rceil$\\
	$q \gets \left\lfloor \frac{N}{M} \right\rfloor$\\
	$r \gets N - M \cdot q$\\
	\QuotientCode{write a $q$-length string of 1 (or 0) bits.\\write a 0 (or 1) bit.\\}
	\RemainderCode{
		\eIf{$r < 2^b -M$}
		{binary code $r$ using $b$ bits.}
		{binary code $r + 2^b - M$ using $b+1$ bits.}
	}
	\textbf{Output:} Golomb code for $N$.
\end{algorithm}

For the particular case $M=2^b,\hspace{0.5em} b \in \mathbb{N}^*$ the codes are called Rice codes and the algorithm is much simpler:

\begin{algorithm}[H]
	\caption{Rice encoding procedure}
	\SetAlgoLined
	\textbf{Input:} $N$\\
	\textbf{Define:} $q \in \mathbb{N}, \quad M = 2^b, \quad b \in \mathbb{N}^*$\\
	$q \gets \left\lfloor \frac{N}{M} \right\rfloor$\\
	\QuotientCode{write a $q$-length string of 1 (or 0) bits.\\write a 0 (or 1) bit.\\}
	write the $b$ \acrshort{lsb} of $N$ in binary.\\
	\textbf{Output:} Rice code for $N$.
\end{algorithm}
